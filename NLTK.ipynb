{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lined-mystery",
   "metadata": {},
   "source": [
    "# NLP on Un Structured Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import *\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import string as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "antique-flash",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Azam/nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Azam\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5d6f072196e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'My name is Muhammd Ahmad Khan'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azam\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \"\"\"\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m     return [\n\u001b[0;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azam\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azam\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azam\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 875\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\azam\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Azam/nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\share\\\\nltk_data'\n    - 'c:\\\\users\\\\azam\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Azam\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text ='My name is Muhammd Ahmad Khan'\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-edward",
   "metadata": {},
   "source": [
    "# 1. Data Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = open('SMSSpamCollection').read() #getting raw data from file using Open Method\n",
    "#Its time for Structuring the data\n",
    "parsed_data=raw_data.replace('\\t','\\n').split('\\n') #replacing every '\\t' with '\\n' and splitting this into array on every '\\n'\n",
    "\n",
    "#Splitting Tags in different List and messages in Different List\n",
    "tags_list=parsed_data[0::2]\n",
    "msgs_list=parsed_data[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame({\n",
    "    'label':tags_list[:-1],\n",
    "    'message':msgs_list\n",
    "})\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Structuring using Pandas \n",
    "dataset = pd.read_csv('SMSSpamCollection', sep='\\t', header=None)\n",
    "dataset.columns=['label','message']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-latter",
   "metadata": {},
   "source": [
    "# 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shape of the Data\n",
    "print(f'Input DataSet has {len(dataset)} rows, and {len(dataset.columns)} columns') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding How many spam and ham there\n",
    "print(f'ham = {len(dataset[dataset[\"label\"]==\"ham\"])}')\n",
    "print(f'spam = {len(dataset[dataset[\"label\"]==\"spam\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Data\n",
    "print(f\"Number of Missing Labels = {dataset['label'].isnull().sum()}\")\n",
    "print(f\"Number of Missing Messages = {dataset['message'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-spray",
   "metadata": {},
   "source": [
    "# 3. Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-orientation",
   "metadata": {},
   "source": [
    "# 1st Step : Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.punctuation #These are all Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to Remove Punctuation from A Word\n",
    "def remove_punctuation(word):\n",
    "    word_nopunc = \"\".join([c for c in word if c not in st.punctuation])\n",
    "    return word_nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Pronunciation Cleaning method on Messages Column\n",
    "dataset['cleaned_message'] = dataset['message'].apply(lambda x : remove_punctuation(x)) \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-artist",
   "metadata": {},
   "source": [
    "# 2nd Step : Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Word_Tokenize() Method to Tokenize all the Messages\n",
    "dataset['cleaned_message_tokens'] = dataset['cleaned_message'].apply(lambda x: word_tokenize(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-release",
   "metadata": {},
   "source": [
    "# 3rd Step : Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all Stopping words from NLTK Library\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove Stopping Words\n",
    "def remove_stopwords(array_of_tokens):\n",
    "    final_array = [word for word in array_of_tokens if word not in stop_words]\n",
    "    return final_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Function on [cleaned_message_tokens]\n",
    "dataset['message_tokens_with_no_stopwords'] = dataset['cleaned_message_tokens'].apply(lambda x : remove_stopwords(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-thermal",
   "metadata": {},
   "source": [
    "# 4th Step : Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-desperate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming Variable\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that Perform Stemming\n",
    "def stemming(message_tokens_with_no_stopwords):\n",
    "    stemmed_words= [ps.stem(word) for word in message_tokens_with_no_stopwords]\n",
    "    return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Function on [message_tokens_with_no_stopwords]\n",
    "dataset['stemmed_words'] = dataset['message_tokens_with_no_stopwords'].apply(lambda x : stemming(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-thumbnail",
   "metadata": {},
   "source": [
    "# 5th Step : Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using WordNet Lemmatizer\n",
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that Perform Lemmatization\n",
    "def lemmatization(message_tokens_with_no_stopwords):\n",
    "    lemmatize_words= [wn.lemmatize(word) for word in message_tokens_with_no_stopwords]\n",
    "    return lemmatize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Function on [message_tokens_with_no_stopwords]\n",
    "dataset['lemmatize_words'] = dataset['message_tokens_with_no_stopwords'].apply(lambda x : lemmatization(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-aquatic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
